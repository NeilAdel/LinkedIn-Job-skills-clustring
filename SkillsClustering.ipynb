{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f02585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\niloa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\niloa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\niloa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install pandas\n",
    "# !pip install nltk kaggle\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a981315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_link</th>\n",
       "      <th>job_skills</th>\n",
       "      <th>job_summary</th>\n",
       "      <th>job_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/housekeeper...</td>\n",
       "      <td>Building Custodial Services, Cleaning, Janitor...</td>\n",
       "      <td>Department:\\nBuilding Custodial Services\\nSala...</td>\n",
       "      <td>Housekeeper I - PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/assistant-g...</td>\n",
       "      <td>Customer service, Restaurant management, Food ...</td>\n",
       "      <td>Summary Of Key Responsibilities\\nThis position...</td>\n",
       "      <td>Assistant General Manager - Huntington 4131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/school-base...</td>\n",
       "      <td>Applied Behavior Analysis (ABA), Data analysis...</td>\n",
       "      <td>Make a difference every day by joining CCRES a...</td>\n",
       "      <td>School-based Behavior Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/electrical-...</td>\n",
       "      <td>Electrical Engineering, Project Controls, Sche...</td>\n",
       "      <td>Requisition ID: 271524\\nRelocation Authorized:...</td>\n",
       "      <td>Electrical Deputy Engineering Group Supervisor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/electrical-...</td>\n",
       "      <td>Electrical Assembly, Point to point wiring, St...</td>\n",
       "      <td>Job Description\\nProduction Specialist\\nElectr...</td>\n",
       "      <td>Electrical Assembly Lead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296376</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/community-a...</td>\n",
       "      <td>Communication Skills, Time Management, Custome...</td>\n",
       "      <td>Job Description\\nThe Community Ambassador is a...</td>\n",
       "      <td>Community Ambassador - The Station at Raleigh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296377</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/sr-it-analy...</td>\n",
       "      <td>Windows SQL, EDI X12, Edifecs Platform, Health...</td>\n",
       "      <td>Interested in fully remote opportunity as a Sr...</td>\n",
       "      <td>Sr. IT Analyst - Edifecs Technical Systems Ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296378</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/operations-...</td>\n",
       "      <td>Adaptability, Communication, Digital Fluency, ...</td>\n",
       "      <td>Entity:\\nProduction &amp; Operations\\nJob Family G...</td>\n",
       "      <td>Operations Excellence Specialist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296379</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/float-patie...</td>\n",
       "      <td>CNA, EMT, BLS, Medical Assistant, CPCT, LPN, R...</td>\n",
       "      <td>Overview To be part of our organization, every...</td>\n",
       "      <td>Float Patient Care Associate CSO YNHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296380</th>\n",
       "      <td>https://www.linkedin.com/jobs/view/conductor-e...</td>\n",
       "      <td>Customer Service, Driving, Loading, Unloading,...</td>\n",
       "      <td>Conductor de entrega de autopartes de medio ti...</td>\n",
       "      <td>Conductor:Entrega a Domicilio - MT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1296381 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  job_link  \\\n",
       "0        https://www.linkedin.com/jobs/view/housekeeper...   \n",
       "1        https://www.linkedin.com/jobs/view/assistant-g...   \n",
       "2        https://www.linkedin.com/jobs/view/school-base...   \n",
       "3        https://www.linkedin.com/jobs/view/electrical-...   \n",
       "4        https://www.linkedin.com/jobs/view/electrical-...   \n",
       "...                                                    ...   \n",
       "1296376  https://www.linkedin.com/jobs/view/community-a...   \n",
       "1296377  https://www.linkedin.com/jobs/view/sr-it-analy...   \n",
       "1296378  https://www.linkedin.com/jobs/view/operations-...   \n",
       "1296379  https://www.linkedin.com/jobs/view/float-patie...   \n",
       "1296380  https://www.linkedin.com/jobs/view/conductor-e...   \n",
       "\n",
       "                                                job_skills  \\\n",
       "0        Building Custodial Services, Cleaning, Janitor...   \n",
       "1        Customer service, Restaurant management, Food ...   \n",
       "2        Applied Behavior Analysis (ABA), Data analysis...   \n",
       "3        Electrical Engineering, Project Controls, Sche...   \n",
       "4        Electrical Assembly, Point to point wiring, St...   \n",
       "...                                                    ...   \n",
       "1296376  Communication Skills, Time Management, Custome...   \n",
       "1296377  Windows SQL, EDI X12, Edifecs Platform, Health...   \n",
       "1296378  Adaptability, Communication, Digital Fluency, ...   \n",
       "1296379  CNA, EMT, BLS, Medical Assistant, CPCT, LPN, R...   \n",
       "1296380  Customer Service, Driving, Loading, Unloading,...   \n",
       "\n",
       "                                               job_summary  \\\n",
       "0        Department:\\nBuilding Custodial Services\\nSala...   \n",
       "1        Summary Of Key Responsibilities\\nThis position...   \n",
       "2        Make a difference every day by joining CCRES a...   \n",
       "3        Requisition ID: 271524\\nRelocation Authorized:...   \n",
       "4        Job Description\\nProduction Specialist\\nElectr...   \n",
       "...                                                    ...   \n",
       "1296376  Job Description\\nThe Community Ambassador is a...   \n",
       "1296377  Interested in fully remote opportunity as a Sr...   \n",
       "1296378  Entity:\\nProduction & Operations\\nJob Family G...   \n",
       "1296379  Overview To be part of our organization, every...   \n",
       "1296380  Conductor de entrega de autopartes de medio ti...   \n",
       "\n",
       "                                                 job_title  \n",
       "0                                       Housekeeper I - PT  \n",
       "1              Assistant General Manager - Huntington 4131  \n",
       "2                            School-based Behavior Analyst  \n",
       "3           Electrical Deputy Engineering Group Supervisor  \n",
       "4                                 Electrical Assembly Lead  \n",
       "...                                                    ...  \n",
       "1296376      Community Ambassador - The Station at Raleigh  \n",
       "1296377  Sr. IT Analyst - Edifecs Technical Systems Ana...  \n",
       "1296378                   Operations Excellence Specialist  \n",
       "1296379              Float Patient Care Associate CSO YNHH  \n",
       "1296380                 Conductor:Entrega a Domicilio - MT  \n",
       "\n",
       "[1296381 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills = pd.read_csv(\"job_skills.csv\")\n",
    "summary = pd.read_csv(\"job_summary.csv\")\n",
    "title = pd.read_csv(\"linkedin_job_postings.csv\", usecols=['job_link', 'job_title'])\n",
    "\n",
    "#merge all the datsets\n",
    "firstdf = pd.merge(skills, summary, on='job_link')\n",
    "df = pd.merge(firstdf, title, on='job_link')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef5ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synonym mapping\n",
    "synonym_map = {\n",
    "    \"customer service\": \"customer support\",\n",
    "    \"customer support\": \"customer support\",\n",
    "    \"customer success\": \"customer support\",\n",
    "    \"technical support\": \"customer support\",\n",
    "    \"project management\": \"project manager\",\n",
    "    \"project manager\": \"project manager\",\n",
    "    \"product management\": \"product manager\",\n",
    "    \"business analysis\": \"business analyst\",\n",
    "    \"data viz\": \"data visualization\",\n",
    "    \"js\": \"javascript\",\n",
    "    \"java\": \"java programming\",\n",
    "    \"java programming\": \"java programming\",\n",
    "    \"c++\": \"c plus plus\",\n",
    "    \"c plus plus\": \"c plus plus\",\n",
    "    \"c#\": \"c sharp\",\n",
    "    \"c sharp\": \"c sharp\",\n",
    "    \"ruby on rails\": \"ruby\",\n",
    "    \"php\": \"php programming\",\n",
    "    \"php programming\": \"php programming\",\n",
    "    \"html\": \"html5\",\n",
    "    \"html5\": \"html5\",\n",
    "    \"css\": \"css3\",\n",
    "    \"css3\": \"css3\",\n",
    "    \"natural language processing\": \"nlp\",\n",
    "    \"software development\": \"software engineering\",\n",
    "    \"software engineer\": \"software engineering\",\n",
    "    \"web development\": \"web developer\",\n",
    "    \"web engineering\": \"web developer\",\n",
    "    \"data engineering\": \"data engineering\",\n",
    "    \"data engineer\": \"data engineering\",\n",
    "    \"cloud engineering\": \"cloud computing\",\n",
    "    \"python programming\": \"python\",\n",
    "    \"python dev\": \"python\",\n",
    "    \"py\": \"python\",\n",
    "    \"data science\": \"data analytics\",\n",
    "    \"data scientist\": \"data analytics\",\n",
    "    \"data analysis\": \"data analytics\",\n",
    "    \"data analyst\": \"data analytics\",\n",
    "    \"machine learning engineer\": \"machine learning\",\n",
    "    \"ml\": \"machine learning\",\n",
    "    \"ai\": \"artificial intelligence\",\n",
    "    \"artificial intelligence\": \"artificial intelligence\",\n",
    "    \"artificial intelligence engineer\": \"artificial intelligence\",\n",
    "    \"cloud computing\": \"cloud computing\",\n",
    "    \"cloud architect\": \"cloud computing\",\n",
    "    \"cloud developer\": \"cloud computing\",\n",
    "    \"full stack developer\": \"full stack developer\",\n",
    "    \"full stack development\": \"full stack developer\",\n",
    "    \"deep learning engineer\": \"deep learning\",\n",
    "    \"deep learning\": \"deep learning\",\n",
    "    \"big data\": \"big data\",\n",
    "    \"sql\": \"structured query language\",\n",
    "    \"statistics\": \"statistical analysis\",\n",
    "    \"excel\": \"microsoft office suite\",\n",
    "    \"ms excel\": \"microsoft office suite\",\n",
    "    \"powerpoint\": \"microsoft office suite\",\n",
    "    \"ms powerpoint\": \"microsoft office suite\",\n",
    "    \"word\": \"microsoft office suite\",\n",
    "    \"ms word\": \"microsoft office suite\",\n",
    "        \"verbal communication\": \"communication\",\n",
    "    \"written communication\": \"communication\",\n",
    "    \"presentation skills\": \"communication\",\n",
    "    \"public speaking\": \"communication\",\n",
    "    \"teamwork\": \"collaboration\",\n",
    "    \"collaboration skills\": \"collaboration\",\n",
    "    \"interpersonal skills\": \"collaboration\",\n",
    "    \"people skills\": \"collaboration\",\n",
    "    \"time management\": \"time_management\",\n",
    "    \"project management\": \"project_management\",\n",
    "    \"leadership skills\": \"leadership\",\n",
    "    \"decision making\": \"decision_making\",\n",
    "    \"critical thinking\": \"problem_solving\",\n",
    "    \"analytical skills\": \"problem_solving\",\n",
    "    \"negotiation skills\": \"negotiation\",\n",
    "    \"customer service\": \"customer_service\",\n",
    "    \"client relations\": \"customer_service\",\n",
    "    \"account management\": \"customer_service\",\n",
    "    \"sales experience\": \"sales\",\n",
    "    \"business development\": \"sales\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed08818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and preprcess the skills data\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "try:\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "try:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Add additional stopwords\n",
    "additional_stopwords = {\"''\", \"...\", \"``\", \"--\"}\n",
    "stop_words = set(nltk_stopwords)\n",
    "stop_words.update(additional_stopwords)\n",
    "\n",
    "\n",
    "def preprocess_skills(skills):\n",
    "    if skills is None:\n",
    "        return []\n",
    "\n",
    "    # Ensure skills is string\n",
    "    skills_str = str(skills) if not isinstance(skills, str) else skills\n",
    "\n",
    "    # Split by comma and \"and\"\n",
    "    skill_list = []\n",
    "    for skill_item in skills_str.split(','):\n",
    "        skill_list.extend([part.strip() for part in re.split(r'\\band\\b', skill_item) if part.strip()])\n",
    "\n",
    "    cleaned_skills = []\n",
    "    punctuation = string.punctuation + \"â€“\"\n",
    "\n",
    "    for skill in skill_list:\n",
    "        # Apply synonym mapping\n",
    "        skill_lower = skill.lower()\n",
    "        for key, value in synonym_map.items():\n",
    "            skill_lower = skill_lower.replace(key, value)\n",
    "\n",
    "        # Remove punctuation, non-letters, extra spaces\n",
    "        skill_lower = ''.join([char for char in skill_lower if char not in punctuation])\n",
    "        skill_lower = re.sub(r'[^a-z\\s]', '', skill_lower)\n",
    "        skill_lower = re.sub(r'\\s+', ' ', skill_lower).strip()\n",
    "\n",
    "        # Tokenize\n",
    "        tokens = skill_lower.split()\n",
    "\n",
    "        # Lemmatize and filter stopwords, short words, and generic terms\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "        tokens = [\n",
    "            word for word in tokens\n",
    "            if word not in stop_words\n",
    "            and re.match(\"^[a-zA-Z]+$\", word)\n",
    "            and len(word) > 3\n",
    "            and word not in {\"ability\", \"skills\", \"skill\"}\n",
    "        ]\n",
    "\n",
    "        cleaned_skills.extend(tokens)\n",
    "\n",
    "    return cleaned_skills\n",
    "\n",
    "\n",
    "df['cleaned_skills'] = df['job_skills'].apply(preprocess_skills)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c05f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (1296381, 1238636)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Filter rare skills\n",
    "skill_counts = Counter(skill for skills in df['cleaned_skills'] for skill in skills)\n",
    "min_freq = 3\n",
    "df['cleaned_skills'] = df['cleaned_skills'].apply(lambda skills: [s for s in skills if skill_counts[s] >= min_freq])\n",
    "\n",
    "# Convert cleaned skills back to string for TF-IDF\n",
    "df['skills_str'] = df['cleaned_skills'].apply(lambda skills: \" \".join(skills))\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.5, stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['skills_str'])\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the processed DataFrame\n",
    "# df.to_csv(\"df_cleaned.csv\", index=False)\n",
    "\n",
    "# with open(\"/tfidf_matrix.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tfidf_matrix, f)\n",
    "\n",
    "# df_cleaned = pd.read_csv(\"/df_cleaned.csv\")\n",
    "\n",
    "# with open(\"/tfidf_matrix.pkl\", \"rb\") as f:\n",
    "#     tfidf_matrix = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb5ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components: 1, Cumulative explained variance: 0.0029\n",
      "Components: 2, Cumulative explained variance: 0.0090\n",
      "Components: 3, Cumulative explained variance: 0.0144\n",
      "Components: 4, Cumulative explained variance: 0.0183\n",
      "Components: 5, Cumulative explained variance: 0.0216\n",
      "Components: 6, Cumulative explained variance: 0.0244\n",
      "Components: 7, Cumulative explained variance: 0.0271\n",
      "Components: 8, Cumulative explained variance: 0.0294\n",
      "Components: 9, Cumulative explained variance: 0.0316\n",
      "Components: 10, Cumulative explained variance: 0.0337\n",
      "Components: 11, Cumulative explained variance: 0.0356\n",
      "Components: 12, Cumulative explained variance: 0.0375\n",
      "Components: 13, Cumulative explained variance: 0.0393\n",
      "Components: 14, Cumulative explained variance: 0.0411\n",
      "Components: 15, Cumulative explained variance: 0.0428\n",
      "Components: 16, Cumulative explained variance: 0.0444\n",
      "Components: 17, Cumulative explained variance: 0.0460\n",
      "Components: 18, Cumulative explained variance: 0.0476\n",
      "Components: 19, Cumulative explained variance: 0.0491\n",
      "Components: 20, Cumulative explained variance: 0.0505\n",
      "Components: 21, Cumulative explained variance: 0.0518\n",
      "Components: 22, Cumulative explained variance: 0.0531\n",
      "Components: 23, Cumulative explained variance: 0.0544\n",
      "Components: 24, Cumulative explained variance: 0.0556\n",
      "Components: 25, Cumulative explained variance: 0.0569\n",
      "Components: 26, Cumulative explained variance: 0.0580\n",
      "Components: 27, Cumulative explained variance: 0.0592\n",
      "Components: 28, Cumulative explained variance: 0.0604\n",
      "Components: 29, Cumulative explained variance: 0.0614\n",
      "Components: 30, Cumulative explained variance: 0.0625\n",
      "Components: 31, Cumulative explained variance: 0.0635\n",
      "Components: 32, Cumulative explained variance: 0.0646\n",
      "Components: 33, Cumulative explained variance: 0.0656\n",
      "Components: 34, Cumulative explained variance: 0.0666\n",
      "Components: 35, Cumulative explained variance: 0.0676\n",
      "Components: 36, Cumulative explained variance: 0.0685\n",
      "Components: 37, Cumulative explained variance: 0.0694\n",
      "Components: 38, Cumulative explained variance: 0.0704\n",
      "Components: 39, Cumulative explained variance: 0.0713\n",
      "Components: 40, Cumulative explained variance: 0.0721\n",
      "Components: 41, Cumulative explained variance: 0.0730\n",
      "Components: 42, Cumulative explained variance: 0.0739\n",
      "Components: 43, Cumulative explained variance: 0.0747\n",
      "Components: 44, Cumulative explained variance: 0.0756\n",
      "Components: 45, Cumulative explained variance: 0.0764\n",
      "Components: 46, Cumulative explained variance: 0.0772\n",
      "Components: 47, Cumulative explained variance: 0.0780\n",
      "Components: 48, Cumulative explained variance: 0.0788\n",
      "Components: 49, Cumulative explained variance: 0.0795\n",
      "Components: 50, Cumulative explained variance: 0.0803\n",
      "Components: 51, Cumulative explained variance: 0.0810\n",
      "Components: 52, Cumulative explained variance: 0.0818\n",
      "Components: 53, Cumulative explained variance: 0.0825\n",
      "Components: 54, Cumulative explained variance: 0.0832\n",
      "Components: 55, Cumulative explained variance: 0.0840\n",
      "Components: 56, Cumulative explained variance: 0.0847\n",
      "Components: 57, Cumulative explained variance: 0.0854\n",
      "Components: 58, Cumulative explained variance: 0.0861\n",
      "Components: 59, Cumulative explained variance: 0.0867\n",
      "Components: 60, Cumulative explained variance: 0.0874\n",
      "Components: 61, Cumulative explained variance: 0.0881\n",
      "Components: 62, Cumulative explained variance: 0.0887\n",
      "Components: 63, Cumulative explained variance: 0.0894\n",
      "Components: 64, Cumulative explained variance: 0.0900\n",
      "Components: 65, Cumulative explained variance: 0.0907\n",
      "Components: 66, Cumulative explained variance: 0.0913\n",
      "Components: 67, Cumulative explained variance: 0.0919\n",
      "Components: 68, Cumulative explained variance: 0.0925\n",
      "Components: 69, Cumulative explained variance: 0.0932\n",
      "Components: 70, Cumulative explained variance: 0.0938\n",
      "Components: 71, Cumulative explained variance: 0.0944\n",
      "Components: 72, Cumulative explained variance: 0.0950\n",
      "Components: 73, Cumulative explained variance: 0.0956\n",
      "Components: 74, Cumulative explained variance: 0.0961\n",
      "Components: 75, Cumulative explained variance: 0.0967\n",
      "Components: 76, Cumulative explained variance: 0.0973\n",
      "Components: 77, Cumulative explained variance: 0.0979\n",
      "Components: 78, Cumulative explained variance: 0.0984\n",
      "Components: 79, Cumulative explained variance: 0.0990\n",
      "Components: 80, Cumulative explained variance: 0.0995\n",
      "Components: 81, Cumulative explained variance: 0.1001\n",
      "Components: 82, Cumulative explained variance: 0.1006\n",
      "Components: 83, Cumulative explained variance: 0.1012\n",
      "Components: 84, Cumulative explained variance: 0.1017\n",
      "Components: 85, Cumulative explained variance: 0.1022\n",
      "Components: 86, Cumulative explained variance: 0.1028\n",
      "Components: 87, Cumulative explained variance: 0.1033\n",
      "Components: 88, Cumulative explained variance: 0.1038\n",
      "Components: 89, Cumulative explained variance: 0.1043\n",
      "Components: 90, Cumulative explained variance: 0.1049\n",
      "Components: 91, Cumulative explained variance: 0.1054\n",
      "Components: 92, Cumulative explained variance: 0.1059\n",
      "Components: 93, Cumulative explained variance: 0.1064\n",
      "Components: 94, Cumulative explained variance: 0.1069\n",
      "Components: 95, Cumulative explained variance: 0.1074\n",
      "Components: 96, Cumulative explained variance: 0.1079\n",
      "Components: 97, Cumulative explained variance: 0.1084\n",
      "Components: 98, Cumulative explained variance: 0.1089\n",
      "Components: 99, Cumulative explained variance: 0.1093\n",
      "Components: 100, Cumulative explained variance: 0.1098\n",
      "Components: 101, Cumulative explained variance: 0.1103\n",
      "Components: 102, Cumulative explained variance: 0.1108\n",
      "Components: 103, Cumulative explained variance: 0.1113\n",
      "Components: 104, Cumulative explained variance: 0.1117\n",
      "Components: 105, Cumulative explained variance: 0.1122\n",
      "Components: 106, Cumulative explained variance: 0.1127\n",
      "Components: 107, Cumulative explained variance: 0.1131\n",
      "Components: 108, Cumulative explained variance: 0.1136\n",
      "Components: 109, Cumulative explained variance: 0.1140\n",
      "Components: 110, Cumulative explained variance: 0.1145\n",
      "Components: 111, Cumulative explained variance: 0.1149\n",
      "Components: 112, Cumulative explained variance: 0.1154\n",
      "Components: 113, Cumulative explained variance: 0.1158\n",
      "Components: 114, Cumulative explained variance: 0.1163\n",
      "Components: 115, Cumulative explained variance: 0.1167\n",
      "Components: 116, Cumulative explained variance: 0.1171\n",
      "Components: 117, Cumulative explained variance: 0.1176\n",
      "Components: 118, Cumulative explained variance: 0.1180\n",
      "Components: 119, Cumulative explained variance: 0.1184\n",
      "Components: 120, Cumulative explained variance: 0.1189\n",
      "Components: 121, Cumulative explained variance: 0.1193\n",
      "Components: 122, Cumulative explained variance: 0.1197\n",
      "Components: 123, Cumulative explained variance: 0.1201\n",
      "Components: 124, Cumulative explained variance: 0.1205\n",
      "Components: 125, Cumulative explained variance: 0.1209\n",
      "Components: 126, Cumulative explained variance: 0.1214\n",
      "Components: 127, Cumulative explained variance: 0.1218\n",
      "Components: 128, Cumulative explained variance: 0.1222\n",
      "Components: 129, Cumulative explained variance: 0.1226\n",
      "Components: 130, Cumulative explained variance: 0.1230\n",
      "Components: 131, Cumulative explained variance: 0.1234\n",
      "Components: 132, Cumulative explained variance: 0.1238\n",
      "Components: 133, Cumulative explained variance: 0.1242\n",
      "Components: 134, Cumulative explained variance: 0.1246\n",
      "Components: 135, Cumulative explained variance: 0.1250\n",
      "Components: 136, Cumulative explained variance: 0.1254\n",
      "Components: 137, Cumulative explained variance: 0.1257\n",
      "Components: 138, Cumulative explained variance: 0.1261\n",
      "Components: 139, Cumulative explained variance: 0.1265\n",
      "Components: 140, Cumulative explained variance: 0.1269\n",
      "Components: 141, Cumulative explained variance: 0.1273\n",
      "Components: 142, Cumulative explained variance: 0.1277\n",
      "Components: 143, Cumulative explained variance: 0.1280\n",
      "Components: 144, Cumulative explained variance: 0.1284\n",
      "Components: 145, Cumulative explained variance: 0.1288\n",
      "Components: 146, Cumulative explained variance: 0.1292\n",
      "Components: 147, Cumulative explained variance: 0.1295\n",
      "Components: 148, Cumulative explained variance: 0.1299\n",
      "Components: 149, Cumulative explained variance: 0.1303\n",
      "Components: 150, Cumulative explained variance: 0.1306\n",
      "Components: 151, Cumulative explained variance: 0.1310\n",
      "Components: 152, Cumulative explained variance: 0.1313\n",
      "Components: 153, Cumulative explained variance: 0.1317\n",
      "Components: 154, Cumulative explained variance: 0.1320\n",
      "Components: 155, Cumulative explained variance: 0.1324\n",
      "Components: 156, Cumulative explained variance: 0.1328\n",
      "Components: 157, Cumulative explained variance: 0.1331\n",
      "Components: 158, Cumulative explained variance: 0.1335\n",
      "Components: 159, Cumulative explained variance: 0.1338\n",
      "Components: 160, Cumulative explained variance: 0.1341\n",
      "Components: 161, Cumulative explained variance: 0.1345\n",
      "Components: 162, Cumulative explained variance: 0.1348\n",
      "Components: 163, Cumulative explained variance: 0.1352\n",
      "Components: 164, Cumulative explained variance: 0.1355\n",
      "Components: 165, Cumulative explained variance: 0.1358\n",
      "Components: 166, Cumulative explained variance: 0.1362\n",
      "Components: 167, Cumulative explained variance: 0.1365\n",
      "Components: 168, Cumulative explained variance: 0.1368\n",
      "Components: 169, Cumulative explained variance: 0.1372\n",
      "Components: 170, Cumulative explained variance: 0.1375\n",
      "Components: 171, Cumulative explained variance: 0.1378\n",
      "Components: 172, Cumulative explained variance: 0.1381\n",
      "Components: 173, Cumulative explained variance: 0.1385\n",
      "Components: 174, Cumulative explained variance: 0.1388\n",
      "Components: 175, Cumulative explained variance: 0.1391\n",
      "Components: 176, Cumulative explained variance: 0.1394\n",
      "Components: 177, Cumulative explained variance: 0.1397\n",
      "Components: 178, Cumulative explained variance: 0.1400\n",
      "Components: 179, Cumulative explained variance: 0.1404\n",
      "Components: 180, Cumulative explained variance: 0.1407\n",
      "Components: 181, Cumulative explained variance: 0.1410\n",
      "Components: 182, Cumulative explained variance: 0.1413\n",
      "Components: 183, Cumulative explained variance: 0.1416\n",
      "Components: 184, Cumulative explained variance: 0.1419\n",
      "Components: 185, Cumulative explained variance: 0.1422\n",
      "Components: 186, Cumulative explained variance: 0.1425\n",
      "Components: 187, Cumulative explained variance: 0.1428\n",
      "Components: 188, Cumulative explained variance: 0.1431\n",
      "Components: 189, Cumulative explained variance: 0.1434\n",
      "Components: 190, Cumulative explained variance: 0.1437\n",
      "Components: 191, Cumulative explained variance: 0.1440\n",
      "Components: 192, Cumulative explained variance: 0.1443\n",
      "Components: 193, Cumulative explained variance: 0.1445\n",
      "Components: 194, Cumulative explained variance: 0.1448\n",
      "Components: 195, Cumulative explained variance: 0.1451\n",
      "Components: 196, Cumulative explained variance: 0.1454\n",
      "Components: 197, Cumulative explained variance: 0.1457\n",
      "Components: 198, Cumulative explained variance: 0.1460\n",
      "Components: 199, Cumulative explained variance: 0.1462\n",
      "Components: 200, Cumulative explained variance: 0.1465\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "# Dimetion Reduction using TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=200, random_state=42)\n",
    "reduced_data = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "cumulative_variance = np.cumsum(svd.explained_variance_ratio_)\n",
    "for i, var in enumerate(cumulative_variance, 1):\n",
    "    print(f\"Components: {i}, Cumulative explained variance: {var:.4f}\")\n",
    "\n",
    "# reduced_data = svd.transform(tfidf_matrix)\n",
    "# pd.DataFrame(reduced_data).to_csv(\"reduced_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d57e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even after the reduction, the shape of the data remains large so sampling is necessary\n",
    "\n",
    "sample_size = 100000\n",
    "indices = np.random.choice(reduced_data.shape[0], sample_size, replace=False)\n",
    "\n",
    "reduced_sample = reduced_data[indices]\n",
    "df_sample = df.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "# pd.DataFrame(reduced_sample).to_csv(\"reduced_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08decf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=200,    \n",
    "    min_samples=50,          \n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom'\n",
    "    \n",
    ")\n",
    "\n",
    "labels = clusterer.fit_predict(reduced_sample)\n",
    "df_sample['cluster'] = labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84956a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: 79819, 0: 200, 1: 287, 2: 235, 3: 19195, 4: 264}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e2081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: ['management', 'communication', 'experience', 'work', 'office', 'collaboration', 'microsoft', 'customerservice', 'intelligencel', 'microsoft office']\n",
      "Cluster 1: ['management', 'communication', 'experience', 'engineering', 'work', 'office', 'microsoft', 'care', 'collaboration', 'data']\n",
      "Cluster 2: ['management', 'communication', 'microsoft', 'experience', 'office', 'collaboration', 'food', 'microsoft office', 'work', 'suite']\n",
      "Cluster 3: ['management', 'communication', 'microsoft', 'office', 'experience', 'work', 'collaboration', 'microsoft office', 'customerservice', 'intelligencel']\n",
      "Cluster 4: ['work', 'communication', 'experience', 'management', 'customerservice', 'office', 'collaboration', 'intelligencel', 'microsoft', 'food']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top_terms_per_cluster(X, labels, terms, n_terms=10):\n",
    "    cluster_terms = {}  # Store top terms for each cluster\n",
    "    for cluster_id in np.unique(labels):\n",
    "        if cluster_id == -1:  # Skip noise\n",
    "            continue\n",
    "        # Get rows belonging to this cluster\n",
    "        cluster_indices = np.where(labels == cluster_id)[0]\n",
    "        cluster_matrix = X[cluster_indices]\n",
    "        # Compute mean TF-IDF for each term in this cluster\n",
    "        mean_tfidf = np.asarray(cluster_matrix.mean(axis=0)).flatten()\n",
    "        # Get indices of top n_terms\n",
    "        top_idx = mean_tfidf.argsort()[-n_terms:][::-1]\n",
    "        # Map indices to feature names\n",
    "        cluster_terms[cluster_id] = [terms[i] for i in top_idx]\n",
    "    return cluster_terms\n",
    "\n",
    "# Example usage\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "cluster_keywords = top_terms_per_cluster(tfidf_matrix, labels, terms, n_terms=10)\n",
    "\n",
    "# Print sample clusters\n",
    "for cluster_id, keywords in cluster_keywords.items():\n",
    "    print(f\"Cluster {cluster_id}: {keywords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f00c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
